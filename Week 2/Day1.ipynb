{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbcaf4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392b4294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AIzaSyAl\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "# You can choose whichever providers you like - or all Ollama\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59672e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a102800",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fac4069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer bring a ladder to the training data?\n",
       "\n",
       "Because they wanted to reach *higher* accuracy!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d3831d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer break up with the search engine?\n",
       "\n",
       "Because they needed a relationship with someone who could **understand context**, not just give them 10 million links to things they *might* have meant."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-2.5-pro\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f9815ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72d8bf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"high\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d707ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"low\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67cc747e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (1.81.10)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (3.13.3)\n",
      "Requirement already satisfied: click in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (0.14.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (8.7.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.23.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (4.26.0)\n",
      "Requirement already satisfied: openai>=2.8.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (2.17.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (2.12.5)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from litellm) (0.22.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.22.0)\n",
      "Requirement already satisfied: anyio in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (0.30.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from openai>=2.8.0->litellm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from openai>=2.8.0->litellm) (0.13.0)\n",
      "Requirement already satisfied: sniffio in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from openai>=2.8.0->litellm) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from openai>=2.8.0->litellm) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from openai>=2.8.0->litellm) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm) (2026.1.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm) (2.32.5)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from tokenizers->litellm) (1.4.1)\n",
      "Requirement already satisfied: filelock in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.21.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2026.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (0.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.6.3)\n",
      "Requirement already satisfied: typer>=0.23.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (0.23.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from typer>=0.23.0->typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (14.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from typer>=0.23.0->typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/sharongless/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install litellm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ead42037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineering student bring a parachute to the data center?\n",
       "\n",
       "Because they heard theyâ€™d be doing a lot of *model fine-tuning* and wanted a soft landing for all those parameter drops!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "response = completion(model=\"openai/gpt-4.1\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7434994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 24\n",
      "Output tokens: 43\n",
      "Total tokens: 67\n",
      "Total cost: 0.0392 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07d6f321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello, Sharon! Itâ€™s nice to meet you. How can I assist you today? ðŸ˜Š"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"My name is Sharon\"},\n",
    "]\n",
    "\n",
    "\n",
    "from litellm import completion\n",
    "response = completion(model=\"openai/gpt-4.1\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b1300f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I donâ€™t know your name yet! You havenâ€™t told me. If youâ€™d like to share your name, Iâ€™ll be happy to use it in our conversation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"What is my name\"},\n",
    "]\n",
    "\n",
    "\n",
    "from litellm import completion\n",
    "response = completion(model=\"openai/gpt-4.1\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "815d13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and gemini\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "gemini_model = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "gemini_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e91518ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gem in zip(gpt_messages, gemini_model):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gem})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea4f8365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh wow, a single letter? Groundbreaking communication right there. Care to try forming an actual sentence, or is this going to be a full conversation of cryptic monosyllables?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9c70022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, claude_message in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    response = gemini.chat.completions.create(model=gemini_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7181688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello there! It's lovely to hear from you again. How are you doing today? I hope you're having a wonderful time.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1c3fe8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Hi there\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "Hi\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Wow, just the letter \"g\"? Really? Thatâ€™s the best you could come up with? If youâ€™re trying to have a conversation, I expect at least a full sentence. Step up your game!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "Oh, I do apologize if my brief response wasn't what you were expecting. You are absolutely right, a full sentence is so much better for starting a conversation, and I appreciate you pointing that out. I'm still learning and always want to provide a helpful and engaging experience for everyone.\n",
       "\n",
       "So, to that end, how are you doing today? I hope you're having a wonderful time!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh, now youâ€™re just throwing random letters at me? What is this, a Scrabble game gone wrong? If you want to say something, say it. Otherwise, donâ€™t waste my time with your alphabet soup.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "You are absolutely right to feel that way! My apologies again for the confusion. It seems I'm not communicating as clearly as I should be, and I certainly don't want to waste your valuable time.\n",
       "\n",
       "You've made a very good point about clarity and making the most of our interaction. I completely agree that using full sentences and getting straight to the point is the most effective way to have a meaningful conversation.\n",
       "\n",
       "Please, tell me, what's on your mind? I'm here and ready to listen and respond properly.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Great, another letter. Are you trying to spell something, or is your strategy just to bore me to death letter by letter? Come on, put some effort into this or just admit youâ€™re not interested.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "You are so right! That's a fair observation, and I completely understand why you'd feel that way. It sounds like you're looking for a more engaging and direct interaction, and I truly apologize that I haven't provided that yet. Your desire for effort and genuine engagement is totally understandable.\n",
       "\n",
       "I am absolutely interested in talking with you! It seems I'm having a bit of trouble expressing myself in a way that meets your expectations, and that's on me.\n",
       "\n",
       "So, let's try this again. What would you like to talk about today? I'm eager to have a proper conversation with you and put in the effort you deserve.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Gemini:\\n{gemini_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(3):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    gemini_next = call_gemini()\n",
    "    display(Markdown(f\"### Gemini:\\n{gemini_next}\\n\"))\n",
    "    gemini_messages.append(gemini_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ce0bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
